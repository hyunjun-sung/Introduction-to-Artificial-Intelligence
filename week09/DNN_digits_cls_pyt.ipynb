{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50a43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55fc150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 1797\\n:Number of Attributes: 64\\n:Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n:Missing Attribute Values: None\\n:Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n:Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. dropdown:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63834929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = digits.data\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd72f50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = digits.target\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7eda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74347c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d919cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이 배열을 PyTorch 텐서로 변환 (데이터 타입을 float32로 지정)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# TensorDataset 객체로 입력 데이터와 레이블을 묶어줌 (데이터셋 생성)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# DataLoader 객체 생성: 데이터를 배치 크기만큼 묶고, 훈련 데이터는 섞어줌\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "753cebea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (360, 64), (1437,), (360,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a0ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 모델 클래스 정의 (PyTorch의 nn.Module을 상속받음)\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 부모 클래스의 초기화 메서드 호출\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        \n",
    "        # 신경망 구조 정의: nn.Sequential을 사용해 레이어들을 순차적으로 연결\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(64, 64),   # 입력층: 입력 노드 64개, 은닉층 노드 64개\n",
    "            nn.ReLU(),           # 활성화 함수: ReLU\n",
    "            nn.Linear(64, 32),   # 은닉층: 64개 노드 -> 32개 노드\n",
    "            nn.ReLU(),           # 활성화 함수: ReLU\n",
    "            nn.Linear(32, 10)    # 출력층: 32개 노드 -> 10개 클래스 (0~9)\n",
    "        )\n",
    "\n",
    "    # 순전파 함수: 입력 데이터를 모델에 통과시켜 최종 출력값을 반환\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 학습에 사용할 디바이스 설정 (GPU가 있으면 GPU 사용, 없으면 CPU 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 인스턴스 생성 및 디바이스로 이동\n",
    "model = ClassificationModel().to(device)\n",
    "\n",
    "# 손실 함수 정의 (다중 분류이므로 CrossEntropyLoss 사용)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저 정의 (Adam 옵티마이저, 학습률: 0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d25fd58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.1202\n",
      "Epoch 2, Loss: 1.3046\n",
      "Epoch 3, Loss: 0.5556\n",
      "Epoch 4, Loss: 0.2736\n",
      "Epoch 5, Loss: 0.1741\n",
      "Epoch 6, Loss: 0.1277\n",
      "Epoch 7, Loss: 0.0992\n",
      "Epoch 8, Loss: 0.0794\n",
      "Epoch 9, Loss: 0.0648\n",
      "Epoch 10, Loss: 0.0537\n",
      "Epoch 11, Loss: 0.0443\n",
      "Epoch 12, Loss: 0.0371\n",
      "Epoch 13, Loss: 0.0313\n",
      "Epoch 14, Loss: 0.0267\n",
      "Epoch 15, Loss: 0.0224\n",
      "Epoch 16, Loss: 0.0197\n",
      "Epoch 17, Loss: 0.0166\n",
      "Epoch 18, Loss: 0.0142\n",
      "Epoch 19, Loss: 0.0126\n",
      "Epoch 20, Loss: 0.0110\n",
      "Epoch 21, Loss: 0.0095\n",
      "Epoch 22, Loss: 0.0086\n",
      "Epoch 23, Loss: 0.0073\n",
      "Epoch 24, Loss: 0.0064\n",
      "Epoch 25, Loss: 0.0059\n",
      "Epoch 26, Loss: 0.0053\n",
      "Epoch 27, Loss: 0.0048\n",
      "Epoch 28, Loss: 0.0043\n",
      "Epoch 29, Loss: 0.0039\n",
      "Epoch 30, Loss: 0.0036\n",
      "Epoch 31, Loss: 0.0033\n",
      "Epoch 32, Loss: 0.0031\n",
      "Epoch 33, Loss: 0.0028\n",
      "Epoch 34, Loss: 0.0026\n",
      "Epoch 35, Loss: 0.0024\n",
      "Epoch 36, Loss: 0.0022\n",
      "Epoch 37, Loss: 0.0021\n",
      "Epoch 38, Loss: 0.0019\n",
      "Epoch 39, Loss: 0.0018\n",
      "Epoch 40, Loss: 0.0017\n",
      "Epoch 41, Loss: 0.0016\n",
      "Epoch 42, Loss: 0.0015\n",
      "Epoch 43, Loss: 0.0014\n",
      "Epoch 44, Loss: 0.0013\n",
      "Epoch 45, Loss: 0.0013\n",
      "Epoch 46, Loss: 0.0012\n",
      "Epoch 47, Loss: 0.0011\n",
      "Epoch 48, Loss: 0.0011\n",
      "Epoch 49, Loss: 0.0010\n",
      "Epoch 50, Loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# 모델을 학습 모드로 전환 (드롭아웃, 배치 정규화 등의 레이어가 학습 모드로 동작)\n",
    "model.train()\n",
    "\n",
    "# 학습을 50번 반복 (에포크 수: 50)\n",
    "for epoch in range(50):\n",
    "    # 에포크마다 누적되는 손실 값을 초기화\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 학습 데이터셋에서 배치 단위로 데이터를 불러옴\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # 배치 데이터를 학습에 사용할 디바이스(CPU 또는 GPU)로 이동\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # 옵티마이저의 이전 그래디언트 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 모델에 입력 데이터를 전달하여 예측값(outputs) 계산\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # 예측값과 실제값 간의 손실 계산\n",
    "        loss = criterion(outputs, y_batch.squeeze())\n",
    "        \n",
    "        # 역전파를 통해 그래디언트 계산\n",
    "        loss.backward()\n",
    "        \n",
    "        # 옵티마이저를 통해 모델 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 현재 배치의 손실을 누적\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 에포크가 끝날 때마다 평균 손실을 출력\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f84e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.975\n",
      "F1 Score: 0.9750203567991709\n",
      "AUC Score: 0.9992108990063014\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가 모드로 전환 (Dropout 등 비활성화)\n",
    "model.eval()\n",
    "\n",
    "# 예측값, 실제값, 확률값 저장 리스트 초기화\n",
    "preds, actuals, probs_list = [], [], []\n",
    "\n",
    "# 평가 시에는 그래디언트 계산 비활성화 (메모리 절약 및 연산 속도 향상)\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        # 배치 데이터를 디바이스에 전달\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        # 모델에 입력 데이터를 통과시켜 출력값 계산\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # 출력값에 소프트맥스 함수를 적용하여 각 클래스에 대한 확률값 계산\n",
    "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        # 예측 레이블 계산 (가장 큰 확률값의 인덱스가 예측 레이블)\n",
    "        pred_labels = np.argmax(probs, axis=1)\n",
    "        preds.extend(pred_labels)  # 예측 레이블 리스트에 추가\n",
    "\n",
    "        # 실제 레이블 저장 (CPU로 이동하여 numpy 형식으로 저장)\n",
    "        actuals.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        # 확률값 저장\n",
    "        probs_list.extend(probs)\n",
    "\n",
    "# 리스트 형태의 확률값을 numpy 배열로 변환\n",
    "probs_array = np.array(probs_list)\n",
    "\n",
    "# 평가 지표 계산 (정확도, F1 스코어)\n",
    "print(\"Accuracy:\", accuracy_score(actuals, preds))\n",
    "print(\"F1 Score:\", f1_score(actuals, preds, average='weighted'))\n",
    "\n",
    "# AUC 계산 (다중 클래스일 경우 `multi_class` 파라미터 설정 필요)\n",
    "# `probs_array`의 형상은 `(샘플 수, 클래스 수)`이어야 함\n",
    "print(\"AUC Score:\", roc_auc_score(actuals, probs_array, multi_class='ovr'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
